# K-Nearest Neighbors (KNN) Algorithm

The **K-Nearest Neighbors (KNN)** algorithm is a simple, supervised machine learning algorithm used for both classification and regression tasks. It classifies a new data point by looking at the 'K' closest points in the dataset and taking a majority vote in the case of classification, or an average in the case of regression.

## How KNN Works

1. **Select the Number of Neighbors (K)**: The user defines the number of neighbors, 'K', that the algorithm considers.
2. **Calculate Distance**: The algorithm calculates the distance between the new data point and all other points in the dataset, often using **Euclidean distance** or **Manhattan distance**.
3. **Identify Neighbors**: KNN finds the 'K' nearest points based on distance.
4. **Make Prediction**:
   - **Classification**: It assigns the most common class among the 'K' neighbors to the new data point.
   - **Regression**: It averages the values of the 'K' neighbors.

## Advantages and Disadvantages

### Advantages
- **Simple and Intuitive**: Easy to understand and implement.
- **No Training Phase**: Since itâ€™s a lazy learner, it only performs computations at prediction time.
- **Effective for Small Datasets**: Works well with smaller datasets and low-dimensional data.

### Disadvantages
- **Computationally Expensive**: Can be slow with large datasets due to distance calculations.
- **Sensitive to Irrelevant Features**: KNN may misclassify if the data has many irrelevant features or dimensions.
- **Requires Feature Scaling**: Distance-based, so features should be scaled (e.g., with normalization or standardization).

## Installation

You can implement KNN using the **scikit-learn** library in Python. Install it using the following command:

```bash
pip install scikit-learn
